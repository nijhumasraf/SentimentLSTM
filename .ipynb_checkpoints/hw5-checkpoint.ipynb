{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428eb489-3951-434b-86a0-5d9bff146ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asraf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6937\n",
      "Epoch [2/20], Loss: 0.6773\n",
      "Epoch [3/20], Loss: 0.6096\n",
      "Epoch [4/20], Loss: 0.5296\n",
      "Epoch [5/20], Loss: 0.4544\n",
      "Epoch [6/20], Loss: 0.3887\n",
      "Epoch [7/20], Loss: 0.3360\n",
      "Epoch [8/20], Loss: 0.2770\n",
      "Epoch [9/20], Loss: 0.2426\n",
      "Epoch [10/20], Loss: 0.2003\n",
      "Epoch [11/20], Loss: 0.1717\n",
      "Epoch [12/20], Loss: 0.1502\n",
      "Epoch [13/20], Loss: 0.1534\n",
      "Epoch [14/20], Loss: 0.1345\n",
      "Epoch [15/20], Loss: 0.1158\n",
      "Epoch [16/20], Loss: 0.1239\n",
      "Epoch [17/20], Loss: 0.1088\n",
      "Epoch [18/20], Loss: 0.1039\n",
      "Epoch [19/20], Loss: 0.1122\n",
      "Epoch [20/20], Loss: 0.0970\n",
      "\n",
      "Test Accuracy: 73.82%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load Dataset\n",
    "try:\n",
    "    amazon_df = pd.read_csv('amazon_cells_labelled.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
    "    imdb_df = pd.read_csv('imdb_labelled.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
    "    yelp_df = pd.read_csv('yelp_labelled.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset files not found. Please ensure the dataset files are in the current directory.\")\n",
    "    exit()\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([amazon_df, imdb_df, yelp_df], ignore_index=True)\n",
    "\n",
    "# Preprocess Text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "df['tokens'] = df['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Build Vocabulary\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "vocab_size = 1000\n",
    "most_common_tokens = Counter(all_tokens).most_common(vocab_size)\n",
    "word_to_idx = {word: idx + 1 for idx, (word, _) in enumerate(most_common_tokens)}  # Start from 1 for padding\n",
    "\n",
    "# Vectorize Tokens\n",
    "def vectorize_tokens(tokens, word_to_idx, max_len=20):\n",
    "    vector = [word_to_idx.get(token, 0) for token in tokens]\n",
    "    if len(vector) < max_len:\n",
    "        vector += [0] * (max_len - len(vector))\n",
    "    else:\n",
    "        vector = vector[:max_len]\n",
    "    return vector\n",
    "\n",
    "max_len = 20\n",
    "df['vector'] = df['tokens'].apply(lambda x: vectorize_tokens(x, word_to_idx, max_len))\n",
    "\n",
    "# Split Data\n",
    "X = np.stack(df['vector'].values)\n",
    "y = df['label'].values\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(df) * split_ratio)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\n",
    "test_data = TensorDataset(torch.tensor(X_test, dtype=torch.long), torch.tensor(y_test, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Define LSTM Model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1, dropout=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Model Parameters\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "\n",
    "model = SentimentLSTM(vocab_size, embed_size, hidden_size, output_size, num_layers)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.view(-1) == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'\\nTest Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
